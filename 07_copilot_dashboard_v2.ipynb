{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba37d7f6",
   "metadata": {},
   "source": [
    "# STREAMLIT DASHBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfaaeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app_v2.py\n",
    "\n",
    "# ============================================\n",
    "# ZENDS COMMUNICATIONS â€” TELECOM COPILOT DASHBOARD\n",
    "# ============================================\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ============================================================\n",
    "# PAGE CONFIG\n",
    "# ============================================================\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"ZENDS COMMUNICATIONS\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "\n",
    "QUEUE_PATH = \"logs/query_queue.csv\"\n",
    "LOG_PATH   = \"logs/query_logs.csv\"\n",
    "\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# SIDEBAR\n",
    "# ============================================================\n",
    "\n",
    "st.sidebar.title(\"ðŸ“¡ ZENDS COMMUNICATIONS\")\n",
    "\n",
    "menu = st.sidebar.radio(\n",
    "\n",
    "    \"Navigation\",\n",
    "\n",
    "    [\n",
    "        \"Customer Copilot\",\n",
    "        \"Live Priority Queue\",\n",
    "        \"Company Analytics\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODELS\n",
    "# ============================================================\n",
    "\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "\n",
    "    # Intent\n",
    "    intent_model = joblib.load(\"models/intent_model.pkl\")\n",
    "    intent_vectorizer = joblib.load(\"models/tfidf.pkl\")\n",
    "\n",
    "    # Sentiment\n",
    "    sentiment_model = joblib.load(\"models/sentiment_model.pkl\")\n",
    "    sentiment_vectorizer = joblib.load(\"models/sentiment_vectorizer.pkl\")\n",
    "    sentiment_decoder = joblib.load(\n",
    "        \"models/sentiment_decoder.pkl\"\n",
    "    )\n",
    "\n",
    "    # Embedding\n",
    "    embedding_model = SentenceTransformer(\n",
    "        \"models/embedding_model\"\n",
    "    )\n",
    "\n",
    "    # RAG\n",
    "    rag = joblib.load(\"models/rag_artifacts.pkl\")\n",
    "\n",
    "    index = rag[\"faiss_index\"]   \n",
    "    chunks = rag[\"chunks\"]\n",
    "\n",
    "    # LLM\n",
    "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=\"models/tinyllama\"\n",
    "    )\n",
    "\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=\"models/tinyllama\",\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        intent_model,\n",
    "        intent_vectorizer,\n",
    "        sentiment_model,\n",
    "        sentiment_vectorizer,\n",
    "        sentiment_decoder,\n",
    "        embedding_model,\n",
    "        index,\n",
    "        chunks,\n",
    "        llm_tokenizer,\n",
    "        llm_model\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    " intent_model,\n",
    " intent_vectorizer,\n",
    " sentiment_model,\n",
    " sentiment_vectorizer,\n",
    " sentiment_decoder,\n",
    " embedding_model,\n",
    " index,\n",
    " chunks,\n",
    " llm_tokenizer,\n",
    " llm_model\n",
    ") = load_models()\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATASET\n",
    "# ============================================================\n",
    "\n",
    "@st.cache_data\n",
    "def load_dataset():\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        \"data/zends_customer_query_dataset.csv\"\n",
    "    )\n",
    "\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "dataset_df = load_dataset()\n",
    "\n",
    "# ============================================================\n",
    "# ML FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def predict_intent(query):\n",
    "\n",
    "    vec = intent_vectorizer.transform([query])\n",
    "    return intent_model.predict(vec)[0]\n",
    "\n",
    "\n",
    "def predict_sentiment(query):\n",
    "\n",
    "    vec = sentiment_vectorizer.transform([query])\n",
    "\n",
    "    pred = sentiment_model.predict(vec)[0]\n",
    "\n",
    "    sentiment = sentiment_decoder[pred]\n",
    "\n",
    "    return sentiment\n",
    "def retrieve_context(query, top_k=3):\n",
    "\n",
    "    emb = embedding_model.encode([query])\n",
    "    emb = np.array(emb).astype(\"float32\")\n",
    "\n",
    "    distances, idx = index.search(emb, top_k)\n",
    "\n",
    "    # Guardrail\n",
    "    if distances[0][0] > 1.2:\n",
    "        return \"NO_CONTEXT_FOUND\"\n",
    "\n",
    "    context = \"\\n\".join(\n",
    "        [chunks[i] for i in idx[0]]\n",
    "    )\n",
    "\n",
    "    return context\n",
    "\n",
    "# ============================================================\n",
    "# PRIORITY CLASSIFIER\n",
    "# ============================================================\n",
    "\n",
    "def assign_priority(sentiment):\n",
    "\n",
    "    if sentiment == \"Frustrated\":\n",
    "        return \"High\"\n",
    "    elif sentiment == \"Informational\":\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# ============================================================\n",
    "# RESPONSE ENGINE (Plug RAG here)\n",
    "# ============================================================\n",
    "\n",
    "def generate_response(query):\n",
    "\n",
    "    context = retrieve_context(query)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a ZENDS Communications telecom customer support assistant.\n",
    "\n",
    "STRICT INSTRUCTIONS:\n",
    "\n",
    "â€¢ Answer ONLY the customer query.\n",
    "â€¢ Do NOT repeat the question.\n",
    "â€¢ Do NOT include words like \"Question\" or \"Answer\".\n",
    "â€¢ Do NOT provide unrelated information.\n",
    "â€¢ If context is insufficient, say the query will be escalated.\n",
    "â€¢ Keep the response professional and concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Customer Query:\n",
    "{query}\n",
    "\n",
    "Final Response:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    inputs = llm_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    outputs = llm_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=120,\n",
    "    min_new_tokens=30,\n",
    "    temperature=0.2,\n",
    "    top_p=0.8,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    eos_token_id=llm_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    text = llm_tokenizer.decode(\n",
    "        outputs[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return text.replace(prompt, \"\").strip()\n",
    "\n",
    "# ============================================================\n",
    "# ADD QUERY TO QUEUE\n",
    "# ============================================================\n",
    "\n",
    "def add_to_queue(user_type, query):\n",
    "\n",
    "    intent = predict_intent(query)\n",
    "    sentiment = predict_sentiment(query)\n",
    "    priority = assign_priority(sentiment)\n",
    "\n",
    "    row = pd.DataFrame([{\n",
    "\n",
    "        \"Time\": datetime.now(),\n",
    "        \"User Type\": user_type,\n",
    "        \"Query\": query,\n",
    "        \"Intent\": intent,\n",
    "        \"Sentiment\": sentiment,\n",
    "        \"Priority\": priority,\n",
    "        \"Status\": \"Pending\"\n",
    "    }])\n",
    "\n",
    "    try:\n",
    "        old = pd.read_csv(QUEUE_PATH)\n",
    "        df = pd.concat([old, row])\n",
    "    except:\n",
    "        df = row\n",
    "\n",
    "    df.to_csv(QUEUE_PATH, index=False)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET TRAFFIC GENERATOR\n",
    "# ============================================================\n",
    "\n",
    "def generate_dataset_queries(n=50):\n",
    "\n",
    "    sampled = dataset_df.sample(n)\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, row in sampled.iterrows():\n",
    "\n",
    "        query = row[\"text\"]\n",
    "\n",
    "        intent = predict_intent(query)\n",
    "        sentiment = predict_sentiment(query)\n",
    "        priority = assign_priority(sentiment)\n",
    "\n",
    "        records.append({\n",
    "\n",
    "            \"Time\": datetime.now(),\n",
    "            \"User Type\": random.choice(\n",
    "                [\"New User\", \"Existing User\"]\n",
    "            ),\n",
    "            \"Query\": query,\n",
    "            \"Intent\": intent,\n",
    "            \"Sentiment\": sentiment,\n",
    "            \"Priority\": priority,\n",
    "            \"Status\": \"Pending\"\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    try:\n",
    "        old = pd.read_csv(QUEUE_PATH)\n",
    "        df = pd.concat([old, df])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df.to_csv(QUEUE_PATH, index=False)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD SORTED QUEUE\n",
    "# ============================================================\n",
    "\n",
    "def load_sorted_queue():\n",
    "\n",
    "    if not os.path.exists(QUEUE_PATH):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(QUEUE_PATH)\n",
    "\n",
    "    order = {\n",
    "        \"High\": 0,\n",
    "        \"Medium\": 1,\n",
    "        \"Low\": 2\n",
    "    }\n",
    "\n",
    "    df[\"Rank\"] = df[\"Priority\"].map(order)\n",
    "\n",
    "    df = df.sort_values(\n",
    "        by=[\"Rank\", \"Time\"]\n",
    "    )\n",
    "\n",
    "    return df.drop(columns=[\"Rank\"])\n",
    "\n",
    "# ============================================================\n",
    "# LOGGING\n",
    "# ============================================================\n",
    "\n",
    "def log_query(user_type, query, response,\n",
    "              intent, sentiment, status,\n",
    "              response_time):\n",
    "\n",
    "    time_now = datetime.now()\n",
    "\n",
    "    row = pd.DataFrame([{\n",
    "        \"Time\": time_now,\n",
    "        \"User Type\": user_type,\n",
    "        \"Query\": query,\n",
    "        \"Response\": response,\n",
    "        \"Intent\": intent,\n",
    "        \"Sentiment\": sentiment,\n",
    "        \"Status\": status,\n",
    "        \"Response Time\": response_time\n",
    "    }])\n",
    "\n",
    "    if os.path.exists(LOG_PATH):\n",
    "        old = pd.read_csv(LOG_PATH)\n",
    "        df = pd.concat([old, row])\n",
    "    else:\n",
    "        df = row\n",
    "\n",
    "    df.to_csv(LOG_PATH, index=False)\n",
    "\n",
    "# ============================================================\n",
    "# PROCESS SINGLE QUERY\n",
    "# ============================================================\n",
    "\n",
    "def process_next_query():\n",
    "\n",
    "    df = load_sorted_queue()\n",
    "\n",
    "    pending = df[df[\"Status\"] == \"Pending\"]\n",
    "\n",
    "    if len(pending) == 0:\n",
    "        return None\n",
    "\n",
    "    # Get next query\n",
    "    row_index = pending.index[0]\n",
    "    row = pending.loc[row_index]\n",
    "\n",
    "    query = row[\"Query\"]\n",
    "    user_type = row[\"User Type\"]\n",
    "    intent = row[\"Intent\"]\n",
    "    sentiment = row[\"Sentiment\"]\n",
    "\n",
    "    # â±ï¸ STEP-2 â†’ START TIMER\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = generate_response(query)\n",
    "\n",
    "    # â±ï¸ END TIMER\n",
    "    end_time = time.time()\n",
    "\n",
    "    response_time = round(end_time - start_time, 2)\n",
    "\n",
    "    # Status\n",
    "    status = (\n",
    "        \"Escalated\"\n",
    "        if \"escalate\" in response.lower()\n",
    "        else \"Resolved\"\n",
    "    )\n",
    "\n",
    "    # Update queue\n",
    "    df.loc[row_index, \"Status\"] = status\n",
    "    df.to_csv(QUEUE_PATH, index=False)\n",
    "\n",
    "    # Log query (Correct fields)\n",
    "    log_query(\n",
    "        user_type,\n",
    "        query,\n",
    "        response,\n",
    "        intent,\n",
    "        sentiment,\n",
    "        status,\n",
    "        response_time\n",
    "    )\n",
    "\n",
    "    return query, response, status\n",
    "# ============================================================\n",
    "# BULK PROCESS\n",
    "# ============================================================\n",
    "\n",
    "def process_bulk_queries(n=10):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _ in range(n):\n",
    "\n",
    "        res = process_next_query()\n",
    "\n",
    "        if res is None:\n",
    "            break\n",
    "\n",
    "        results.append(res)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ============================================================\n",
    "# AUTO SOLVE\n",
    "# ============================================================\n",
    "\n",
    "def auto_solve_queue(interval=2, limit=50):\n",
    "\n",
    "    processed = []\n",
    "\n",
    "    for _ in range(limit):\n",
    "\n",
    "        res = process_next_query()\n",
    "\n",
    "        if res is None:\n",
    "            break\n",
    "\n",
    "        processed.append(res)\n",
    "\n",
    "        time.sleep(interval)\n",
    "\n",
    "    return processed\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ‘¤ CUSTOMER COPILOT\n",
    "# ============================================================\n",
    "\n",
    "if menu == \"Customer Copilot\":\n",
    "\n",
    "    st.title(\"ðŸ‘¤ Customer AI Assistant\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # SESSION STATE INIT\n",
    "    # -------------------------------\n",
    "    if \"query_text\" not in st.session_state:\n",
    "        st.session_state[\"query_text\"] = \"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # USER TYPE\n",
    "    # -------------------------------\n",
    "    user_type = st.radio(\n",
    "        \"User Type\",\n",
    "        [\"New User\", \"Existing User\"]\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # QUERY INPUT (Auto-fill enabled)\n",
    "    # -------------------------------\n",
    "    query = st.text_area(\n",
    "        \"Enter telecom query\",\n",
    "        value=st.session_state[\"query_text\"],\n",
    "        key=\"query_box\"\n",
    "    )\n",
    "\n",
    "    # ========================================================\n",
    "    # ðŸ” TOP 10 FAQs FROM LOGS\n",
    "    # ========================================================\n",
    "\n",
    "    st.markdown(\"## ðŸ” Top Asked Queries\")\n",
    "\n",
    "    if os.path.exists(LOG_PATH):\n",
    "\n",
    "     logs_df = pd.read_csv(LOG_PATH)\n",
    "\n",
    "     top_queries = (\n",
    "        logs_df[\"Query\"]\n",
    "        .value_counts()\n",
    "        .head(10)\n",
    "     )\n",
    "\n",
    "     cols = st.columns(2)\n",
    "\n",
    "     for i, q in enumerate(top_queries.index):\n",
    "\n",
    "        if cols[i % 2].button(\n",
    "            f\"ðŸ“Œ {q}\",\n",
    "            key=f\"topfaq_{i}\"\n",
    "        ):\n",
    "\n",
    "            # ðŸ”¥ STEP 1 â€” Fill textbox\n",
    "            st.session_state[\"query_text\"] = q\n",
    "\n",
    "            # ðŸ”¥ STEP 2 â€” Rerun UI\n",
    "            st.rerun()\n",
    "\n",
    "    else:\n",
    "     st.info(\"No query logs available yet.\")\n",
    "\n",
    "    # ========================================================\n",
    "    # GENERATE RESPONSE\n",
    "    # ========================================================\n",
    "\n",
    "    if st.button(\"Generate Response\"):\n",
    "\n",
    "        query = st.session_state[\"query_text\"]\n",
    "\n",
    "        if query:\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            intent = predict_intent(query)\n",
    "            sentiment = predict_sentiment(query)\n",
    "            priority = assign_priority(sentiment)\n",
    "\n",
    "            response = generate_response(query)\n",
    "\n",
    "            response_time = round(\n",
    "                time.time() - start_time, 2\n",
    "            )\n",
    "\n",
    "            status = (\n",
    "                \"Escalated\"\n",
    "                if \"escalate\" in response.lower()\n",
    "                else \"Resolved\"\n",
    "            )\n",
    "\n",
    "            # ---------------------------\n",
    "            # SHOW RESPONSE\n",
    "            # ---------------------------\n",
    "            st.success(response)\n",
    "\n",
    "            # ---------------------------\n",
    "            # INSIGHTS\n",
    "            # ---------------------------\n",
    "            st.markdown(\"### Insights\")\n",
    "\n",
    "            c1, c2, c3 = st.columns(3)\n",
    "\n",
    "            c1.metric(\"Intent\", intent)\n",
    "            c2.metric(\"Sentiment\", sentiment)\n",
    "            c3.metric(\"Priority\", priority)\n",
    "\n",
    "            # ---------------------------\n",
    "            # LOG\n",
    "            # ---------------------------\n",
    "            log_query(\n",
    "                user_type,\n",
    "                query,\n",
    "                response,\n",
    "                intent,\n",
    "                sentiment,\n",
    "                status,\n",
    "                response_time\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            st.warning(\"Please enter a query.\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸš¦ LIVE PRIORITY QUEUE\n",
    "# ============================================================\n",
    "\n",
    "elif menu == \"Live Priority Queue\":\n",
    "\n",
    "    st.title(\"ðŸš¦ Live Priority Queue\")\n",
    "\n",
    "    # ========================================================\n",
    "    # ðŸ“Š TRAFFIC GENERATOR\n",
    "    # ========================================================\n",
    "\n",
    "    st.subheader(\"ðŸ“Š Simulate Incoming Traffic\")\n",
    "\n",
    "    col1, col2 = st.columns([3, 1])\n",
    "\n",
    "    with col1:\n",
    "\n",
    "        traffic_n = st.slider(\n",
    "            \"Generate Dataset Queries\",\n",
    "            min_value=10,\n",
    "            max_value=500,\n",
    "            value=50,\n",
    "            step=10\n",
    "        )\n",
    "\n",
    "    with col2:\n",
    "\n",
    "        if st.button(\"Generate Traffic\"):\n",
    "\n",
    "            generate_dataset_queries(traffic_n)\n",
    "\n",
    "            st.success(\n",
    "                f\"{traffic_n} queries added to queue.\"\n",
    "            )\n",
    "\n",
    "            st.rerun()\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "\n",
    "    # ========================================================\n",
    "    # LOAD QUEUE\n",
    "    # ========================================================\n",
    "\n",
    "    if os.path.exists(QUEUE_PATH):\n",
    "\n",
    "        df = load_sorted_queue()\n",
    "\n",
    "        # ====================================================\n",
    "        # KPI METRICS\n",
    "        # ====================================================\n",
    "\n",
    "        pending = (df[\"Status\"] == \"Pending\").sum()\n",
    "        resolved = (df[\"Status\"] == \"Resolved\").sum()\n",
    "        escalated = (df[\"Status\"] == \"Escalated\").sum()\n",
    "\n",
    "        high_priority = (df[\"Priority\"] == \"High\").sum()\n",
    "\n",
    "        k1, k2, k3, k4 = st.columns(4)\n",
    "\n",
    "        k1.metric(\"Pending\", pending)\n",
    "        k2.metric(\"Resolved\", resolved)\n",
    "        k3.metric(\"Escalated\", escalated)\n",
    "        k4.metric(\"High Priority\", high_priority)\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # ====================================================\n",
    "        # PROCESSING MODE SELECTOR\n",
    "        # ====================================================\n",
    "\n",
    "        st.subheader(\"âš™ï¸ Processing Mode\")\n",
    "\n",
    "        mode = st.radio(\n",
    "            \"Select Queue Processing Mode\",\n",
    "            [\n",
    "                \"Manual (Agent)\",\n",
    "                \"Bulk (Team)\",\n",
    "                \"Auto AI (Copilot)\"\n",
    "            ],\n",
    "            horizontal=True\n",
    "        )\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # ====================================================\n",
    "        # QUEUE TABLE\n",
    "        # ====================================================\n",
    "\n",
    "        st.subheader(\"ðŸ“‹ Live Query Queue\")\n",
    "\n",
    "        st.dataframe(\n",
    "            df,\n",
    "            use_container_width=True\n",
    "        )\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # ====================================================\n",
    "        # MODE 1 â€” MANUAL PROCESSING\n",
    "        # ====================================================\n",
    "\n",
    "        if mode == \"Manual (Agent)\":\n",
    "\n",
    "            st.markdown(\"### ðŸ‘¤ Agent Processing\")\n",
    "\n",
    "            if st.button(\"Process Next Query\"):\n",
    "\n",
    "                res = process_next_query()\n",
    "\n",
    "                if res:\n",
    "\n",
    "                    st.success(\n",
    "                        f\"Solved: {res[0]}\"\n",
    "                    )\n",
    "\n",
    "                    st.rerun()\n",
    "\n",
    "                else:\n",
    "                    st.warning(\"No pending queries.\")\n",
    "\n",
    "        # ====================================================\n",
    "        # MODE 2 â€” BULK PROCESSING\n",
    "        # ====================================================\n",
    "\n",
    "        elif mode == \"Bulk (Team)\":\n",
    "\n",
    "            st.markdown(\"### ðŸ‘¥ Bulk Processing\")\n",
    "\n",
    "            bulk_n = st.slider(\n",
    "                \"Queries to Process\",\n",
    "                1, 50, 10\n",
    "            )\n",
    "\n",
    "            if st.button(\"Process Bulk Queries\"):\n",
    "\n",
    "                results = process_bulk_queries(\n",
    "                    bulk_n\n",
    "                )\n",
    "\n",
    "                st.success(\n",
    "                    f\"{len(results)} queries solved.\"\n",
    "                )\n",
    "\n",
    "                st.rerun()\n",
    "\n",
    "        # ====================================================\n",
    "        # MODE 3 â€” AUTO AI PROCESSING\n",
    "        # ====================================================\n",
    "\n",
    "        elif mode == \"Auto AI (Copilot)\":\n",
    "\n",
    "            st.markdown(\"### ðŸ¤– Autonomous AI Processing\")\n",
    "\n",
    "            auto_limit = st.slider(\n",
    "                \"Max Queries to Auto-Solve\",\n",
    "                5, 200, 50\n",
    "            )\n",
    "\n",
    "            interval = st.slider(\n",
    "                \"Response Interval (sec)\",\n",
    "                1, 5, 2\n",
    "            )\n",
    "\n",
    "            if st.button(\"Start Auto Solving\"):\n",
    "\n",
    "                progress = st.progress(0)\n",
    "\n",
    "                solved = 0\n",
    "\n",
    "                for i in range(auto_limit):\n",
    "\n",
    "                    res = process_next_query()\n",
    "\n",
    "                    if res is None:\n",
    "                        break\n",
    "\n",
    "                    solved += 1\n",
    "\n",
    "                    progress.progress(\n",
    "                        (i + 1) / auto_limit\n",
    "                    )\n",
    "\n",
    "                    time.sleep(interval)\n",
    "\n",
    "                st.success(\n",
    "                    f\"{solved} queries auto-solved.\"\n",
    "                )\n",
    "\n",
    "                st.rerun()\n",
    "\n",
    "            # ================================================\n",
    "            # CONTINUOUS AUTO MODE\n",
    "            # ================================================\n",
    "\n",
    "            auto_live = st.toggle(\n",
    "                \"Enable Continuous Auto Mode\"\n",
    "            )\n",
    "\n",
    "            if auto_live:\n",
    "\n",
    "                pending_df = df[\n",
    "                    df[\"Status\"] == \"Pending\"\n",
    "                ]\n",
    "\n",
    "                if len(pending_df) > 0:\n",
    "\n",
    "                    st.info(\n",
    "                        \"AI Copilot solving pending queries...\"\n",
    "                    )\n",
    "\n",
    "                    process_next_query()\n",
    "\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    st.rerun()\n",
    "\n",
    "    else:\n",
    "        st.warning(\"No queries in queue yet.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ¢ COMPANY ANALYTICS\n",
    "# ============================================================\n",
    "\n",
    "elif menu == \"Company Analytics\":\n",
    "\n",
    "    st.title(\"ðŸ¢ Company Analytics\")\n",
    "\n",
    "    if os.path.exists(LOG_PATH):\n",
    "\n",
    "        df = pd.read_csv(LOG_PATH)\n",
    "\n",
    "        # Ensure numeric\n",
    "        df[\"Response Time\"] = pd.to_numeric(\n",
    "            df[\"Response Time\"],\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # ==============================\n",
    "        # KPI ROW\n",
    "        # ==============================\n",
    "\n",
    "        k1, k2, k3, k4, k5 = st.columns(5)\n",
    "\n",
    "        k1.metric(\"Total Queries\", len(df))\n",
    "\n",
    "        k2.metric(\n",
    "            \"Resolved\",\n",
    "            (df[\"Status\"] == \"Resolved\").sum()\n",
    "        )\n",
    "\n",
    "        k3.metric(\n",
    "            \"Escalated\",\n",
    "            (df[\"Status\"] == \"Escalated\").sum()\n",
    "        )\n",
    "\n",
    "        k4.metric(\n",
    "            \"Avg Response Time\",\n",
    "            f\"{df['Response Time'].mean():.2f} sec\"\n",
    "        )\n",
    "\n",
    "        frustrated_pct = (\n",
    "            (df[\"Sentiment\"] == \"Frustrated\").sum()\n",
    "            / len(df) * 100\n",
    "        )\n",
    "\n",
    "        k5.metric(\n",
    "            \"Frustrated %\",\n",
    "            f\"{frustrated_pct:.1f}%\"\n",
    "        )\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # ==============================\n",
    "        # RESPONSE TIME METRICS\n",
    "        # ==============================\n",
    "\n",
    "        r1, r2, r3 = st.columns(3)\n",
    "\n",
    "        r1.metric(\n",
    "            \"Max Response Time\",\n",
    "            f\"{df['Response Time'].max():.2f} sec\"\n",
    "        )\n",
    "\n",
    "        r2.metric(\n",
    "            \"Min Response Time\",\n",
    "            f\"{df['Response Time'].min():.2f} sec\"\n",
    "        )\n",
    "\n",
    "        sla_breach = (\n",
    "            df[\"Response Time\"] > 30\n",
    "        ).sum()\n",
    "\n",
    "        r3.metric(\n",
    "            \"SLA Breaches (>30s)\",\n",
    "            sla_breach\n",
    "        )\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # ==============================\n",
    "        # INTENT DISTRIBUTION\n",
    "        # ==============================\n",
    "\n",
    "        c1, c2 = st.columns(2)\n",
    "\n",
    "        with c1:\n",
    "\n",
    "            st.subheader(\"Intent Distribution\")\n",
    "\n",
    "            intent_counts = df[\"Intent\"].value_counts()\n",
    "\n",
    "            fig1, ax1 = plt.subplots()\n",
    "\n",
    "            bars = ax1.bar(\n",
    "                intent_counts.index,\n",
    "                intent_counts.values\n",
    "            )\n",
    "\n",
    "            for bar in bars:\n",
    "                ax1.text(\n",
    "                    bar.get_x()+bar.get_width()/2,\n",
    "                    bar.get_height(),\n",
    "                    int(bar.get_height()),\n",
    "                    ha=\"center\"\n",
    "                )\n",
    "\n",
    "            st.pyplot(fig1, use_container_width=True)\n",
    "\n",
    "        # ==============================\n",
    "        # SENTIMENT DISTRIBUTION\n",
    "        # ==============================\n",
    "\n",
    "        with c2:\n",
    "\n",
    "            st.subheader(\"Sentiment Distribution\")\n",
    "\n",
    "            sent_counts = df[\"Sentiment\"].value_counts()\n",
    "\n",
    "            fig2, ax2 = plt.subplots()\n",
    "\n",
    "            bars2 = ax2.bar(\n",
    "                sent_counts.index,\n",
    "                sent_counts.values\n",
    "            )\n",
    "\n",
    "            for bar in bars2:\n",
    "                ax2.text(\n",
    "                    bar.get_x()+bar.get_width()/2,\n",
    "                    bar.get_height(),\n",
    "                    int(bar.get_height()),\n",
    "                    ha=\"center\"\n",
    "                )\n",
    "\n",
    "            st.pyplot(fig2, use_container_width=True)\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # ==============================\n",
    "        # RESPONSE TIME TREND\n",
    "        # ==============================\n",
    "\n",
    "        st.subheader(\"ðŸ“ˆ Response Time Trend\")\n",
    "\n",
    "        df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n",
    "        df[\"Response Time\"] = pd.to_numeric(\n",
    "            df[\"Response Time\"],\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        trend_df = df[[\"Time\", \"Response Time\"]].copy()\n",
    "        trend_df = trend_df.set_index(\"Time\")\n",
    "\n",
    "        # Aggregation selector\n",
    "        agg_level = st.selectbox(\n",
    "            \"Aggregation Level\",\n",
    "            [\"Hourly\", \"Daily\"]\n",
    "        )\n",
    "\n",
    "        if agg_level == \"Hourly\":\n",
    "            trend_df = trend_df.resample(\"H\").mean()\n",
    "            date_format = \"%d %b %H:%M\"\n",
    "        else:\n",
    "            trend_df = trend_df.resample(\"D\").mean()\n",
    "            date_format = \"%d %b\"\n",
    "\n",
    "        trend_df = trend_df.dropna()\n",
    "\n",
    "        # Rolling average (Smoothed)\n",
    "        trend_df[\"Rolling Avg\"] = (\n",
    "            trend_df[\"Response Time\"]\n",
    "            .rolling(window=3)\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        # ==========================================================\n",
    "        # Plot\n",
    "        # ==========================================================\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "        # Raw line\n",
    "        ax.plot(\n",
    "            trend_df.index,\n",
    "            trend_df[\"Response Time\"],\n",
    "            alpha=0.4,\n",
    "            label=\"Raw\"\n",
    "        )\n",
    "\n",
    "        # Smoothed line\n",
    "        ax.plot(\n",
    "            trend_df.index,\n",
    "            trend_df[\"Rolling Avg\"],\n",
    "            linewidth=3,\n",
    "            label=\"Smoothed\"\n",
    "        )\n",
    "\n",
    "        # SLA line\n",
    "        ax.axhline(\n",
    "            y=30,\n",
    "            linestyle=\"--\",\n",
    "            label=\"SLA (30s)\"\n",
    "        )\n",
    "\n",
    "        # -------------------------------\n",
    "        # X-axis Formatting\n",
    "        # -------------------------------\n",
    "\n",
    "        ax.xaxis.set_major_formatter(\n",
    "            mdates.DateFormatter(date_format)\n",
    "        )\n",
    "\n",
    "        ax.xaxis.set_major_locator(\n",
    "            mdates.AutoDateLocator()\n",
    "        )\n",
    "\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "        ax.set_ylabel(\"Response Time (sec)\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "        st.pyplot(fig, use_container_width=True)\n",
    "\n",
    "        # ==============================\n",
    "        # LOG TABLE\n",
    "        # ==============================\n",
    "\n",
    "        st.subheader(\"Query Logs\")\n",
    "\n",
    "        st.dataframe(\n",
    "            df,\n",
    "            use_container_width=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        st.warning(\"No logs available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
